{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxsMFSlc/xgvRyi2zeguqo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreesh09/Detection-of-textual-CyberBullying-using-ML/blob/main/Copy_of_Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries"
      ],
      "metadata": {
        "id": "3IABYOy3BgA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x16XC25fBjHd",
        "outputId": "aa42b6ba-4231-469c-a28d-9ad8405b6e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "NePLJUrjBkRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "dataset1 = pd.read_csv(\"twitter_racism_parsed_dataset.csv\")\n",
        "dataset2 = pd.read_csv(\"twitter_sexism_parsed_dataset.csv\")\n",
        "datasets = [dataset1, dataset2]"
      ],
      "metadata": {
        "id": "UqQvxE3eDRBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove special characters, numbers, and URLs\n",
        "def remove(data):\n",
        "  data.loc[:, 'Text'] = data.loc[:, 'Text'].replace('[^a-zA-Z]', ' ', regex=True)\n",
        "  return data"
      ],
      "metadata": {
        "id": "-j5ZXscvDyN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase the text\n",
        "def LC(data):\n",
        "  data.loc[:, 'Text'] = data.loc[:, 'Text'].apply(lambda x: x.lower())\n",
        "  return data"
      ],
      "metadata": {
        "id": "eIS0IBa7D9wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "def removeSW(data):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  data.loc[:, 'Text'] = data.loc[:, 'Text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "  return data"
      ],
      "metadata": {
        "id": "8rtMfBaEFd3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform stemming\n",
        "def Stem(data):\n",
        "  stemmer = SnowballStemmer('english')\n",
        "  data.loc[:, 'Text'] = data.loc[:, 'Text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\n",
        "  return data"
      ],
      "metadata": {
        "id": "3bS1aXc4FfFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Empty(df):\n",
        "  #removing Empty values\n",
        "  df.dropna(subset=['Text'], inplace=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "1ZnkmFgCDINH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Duplicate(df):\n",
        "  #Removing duplicates\n",
        "  df = df.drop_duplicates(subset='Text', keep='first')\n",
        "  return df"
      ],
      "metadata": {
        "id": "YdjpqqIo040i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in datasets:\n",
        "  dataset = Empty(dataset)\n",
        "  dataset = remove(dataset)\n",
        "  dataset = LC(dataset)\n",
        "  dataset = removeSW(dataset)\n",
        "  dataset = Stem(dataset)\n",
        "  dataset = Duplicate(dataset)"
      ],
      "metadata": {
        "id": "Aa2UlWCQloJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(datasets[1].iloc[2,2])"
      ],
      "metadata": {
        "id": "ftigzVN01Gad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting of Dataset"
      ],
      "metadata": {
        "id": "rZvXvib5webG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "vectorizer = TfidfVectorizer(lowercase=False, use_idf=True)\n",
        "vectTrain = vectorizer.fit_transform((datasets[0])['Text'])\n",
        "X = vectTrain\n",
        "Y = (datasets[0])[\"oh_label\"].values\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pn0R4UMtwxgY",
        "outputId": "29994cfb-b938-47dd-ff4c-e5c0fd62312a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nvectorizer = TfidfVectorizer(lowercase=False, use_idf=True)\\nvectTrain = vectorizer.fit_transform((datasets[0])[\\'Text\\'])\\nX = vectTrain\\nY = (datasets[0])[\"oh_label\"].values\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 334
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def Split(data):\n",
        "X = (datasets[0])[\"Text\"].values\n",
        "Y = (datasets[0])[\"oh_label\"].values\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y,test_size=0.20)"
      ],
      "metadata": {
        "id": "EJjDTeRi7Blc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding Categorical values"
      ],
      "metadata": {
        "id": "W8tYAneL1l12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(lowercase=False, use_idf=True)\n",
        "vectTrain = vectorizer.fit_transform(X_train)\n",
        "vectTest = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "0H7P4T2461t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model"
      ],
      "metadata": {
        "id": "BybCv6u61R96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "classifier = svm.SVC(kernel=\"linear\")\n",
        "\n",
        "#fitting training data into the algorithm \n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "#predicting the test data labels\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "#getting the acuuracy score \n",
        "print(\"SVM accuracy score: \", accuracy_score(predictions, Y_test)*100)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5lfqLl0s1T25",
        "outputId": "b2277b8b-42e8-4823-b96a-0b56cf1c5517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclassifier = svm.SVC(kernel=\"linear\")\\n\\n#fitting training data into the algorithm \\nclassifier.fit(X_train, Y_train)\\n\\n#predicting the test data labels\\npredictions = classifier.predict(X_test)\\n\\n#getting the acuuracy score \\nprint(\"SVM accuracy score: \", accuracy_score(predictions, Y_test)*100)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 337
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = svm.SVC(kernel=\"linear\")\n",
        "\n",
        "#fitting training data into the algorithm \n",
        "classifier.fit(vectTrain, Y_train)\n",
        "\n",
        "#predicting the test data labels\n",
        "predictions = classifier.predict(vectTest)\n",
        "\n",
        "#getting the acuuracy score \n",
        "print(\"SVM accuracy score: \", accuracy_score(predictions, Y_test)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taHtZ9Um7u7W",
        "outputId": "76a6cfdb-f5aa-41bb-e369-16a547f78d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM accuracy score:  93.06122448979592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'Text': [\"Islam\"]}\n",
        "d = pd.DataFrame(data)\n",
        "print(d)\n",
        "d = Empty(d)\n",
        "d = remove(d)\n",
        "d = LC(d)\n",
        "d = removeSW(d)\n",
        "d = Stem(d)\n",
        "d = Duplicate(d)\n",
        "#print(d)\n",
        "vectP = vectorizer.transform(d['Text'])\n",
        "print(vectP)\n",
        "print(classifier.predict(vectP))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj_-StpX2A53",
        "outputId": "23ad7a96-ed65-4e92-c9ba-18da3e80c41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Text\n",
            "0  Islam\n",
            "  (0, 6220)\t1.0\n",
            "[1]\n"
          ]
        }
      ]
    }
  ]
}