{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEKH4QCdixoJFuZExAOb7q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreesh09/Detection-of-textual-CyberBullying-using-ML/blob/main/CyberBullying-Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries"
      ],
      "metadata": {
        "id": "3IABYOy3BgA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x16XC25fBjHd",
        "outputId": "3cfca5b0-580e-4aa8-ccff-f2c4560ffadd"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "NePLJUrjBkRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "dataset1 = pd.read_csv(\"twitter_racism_parsed_dataset.csv\")\n",
        "dataset2 = pd.read_csv(\"twitter_sexism_parsed_dataset.csv\")\n",
        "datasets = [dataset1, dataset2]"
      ],
      "metadata": {
        "id": "UqQvxE3eDRBm"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove special characters, numbers, and URLs\n",
        "def remove(data):\n",
        "  data.loc[:, 'Text'] = data.loc[:, 'Text'].replace('[^a-zA-Z]', ' ', regex=True)\n",
        "  return data"
      ],
      "metadata": {
        "id": "-j5ZXscvDyN2"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase the text\n",
        "def LC(data):\n",
        "  data.loc[:, 'Text'] = data.loc[:, 'Text'].apply(lambda x: x.lower())\n",
        "  return data"
      ],
      "metadata": {
        "id": "eIS0IBa7D9wA"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "def removeSW(data):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  data.loc[:, 'Text'] = data.loc[:, 'Text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "  return data"
      ],
      "metadata": {
        "id": "8rtMfBaEFd3Z"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform stemming\n",
        "def Stem(data):\n",
        "  stemmer = SnowballStemmer('english')\n",
        "  data.loc[:, 'Text'] = data.loc[:, 'Text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\n",
        "  return data"
      ],
      "metadata": {
        "id": "3bS1aXc4FfFg"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Empty(df):\n",
        "  #removing Empty values\n",
        "  df.dropna(subset=['Text'], inplace=True)\n",
        "  df.dropna(subset=['oh_label'], inplace=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "1ZnkmFgCDINH"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Duplicate(df):\n",
        "  #Removing duplicates\n",
        "  df = df.drop_duplicates(subset='Text', keep='first')\n",
        "  return df"
      ],
      "metadata": {
        "id": "YdjpqqIo040i"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in datasets:\n",
        "  dataset = Empty(dataset)\n",
        "  dataset = remove(dataset)\n",
        "  dataset = LC(dataset)\n",
        "  dataset = removeSW(dataset)\n",
        "  dataset = Stem(dataset)\n",
        "  dataset = Duplicate(dataset)"
      ],
      "metadata": {
        "id": "Aa2UlWCQloJV"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(datasets[1].iloc[2, 2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFCa-sYlOg_K",
        "outputId": "0eed3f26-1968-46e3-e491-7061a0ee3f8d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rt eccl everyon underestim us still well underestim judg mkr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting of Dataset"
      ],
      "metadata": {
        "id": "rZvXvib5webG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Split(data):\n",
        "  X = data.loc[:, \"Text\"].values\n",
        "  Y = data.loc[:, \"oh_label\"].values\n",
        "  X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y,test_size=0.20)\n",
        "  splits = {'X_train': X_train, 'X_test': X_test, 'Y_train': Y_train, 'Y_test': Y_test}\n",
        "  return splits\n"
      ],
      "metadata": {
        "id": "EJjDTeRi7Blc"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding Categorical values"
      ],
      "metadata": {
        "id": "W8tYAneL1l12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fitVectorizer(X):\n",
        "  vectorizer = TfidfVectorizer(lowercase=False, use_idf=True)\n",
        "  vectorizer.fit(X)\n",
        "  return vectorizer\n",
        "\n",
        "def Encode(X, vectorizer):\n",
        "  vect = vectorizer.transform(X)\n",
        "  return vect"
      ],
      "metadata": {
        "id": "0H7P4T2461t3"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model"
      ],
      "metadata": {
        "id": "BybCv6u61R96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "vectTests = []\n",
        "Y_tests = []\n",
        "# Iterate through the datasets\n",
        "for dataset in datasets:\n",
        "    # Split the dataset into training and test sets\n",
        "    df = Split(dataset)\n",
        "    #print(df.iloc[0,0])\n",
        "    X_train = df['X_train']\n",
        "    X_test = df['X_test']\n",
        "    Y_train = df['Y_train']\n",
        "    Y_test = df['Y_test']\n",
        "\n",
        "\n",
        "    vectorizer = fitVectorizer(X_train)\n",
        "    vectTrain = Encode(X_train, vectorizer)\n",
        "    vectTest = Encode(X_test, vectorizer)\n",
        "    \n",
        "    model = svm.SVC(kernel=\"linear\")\n",
        "\n",
        "    #fitting training data into the algorithm \n",
        "    model.fit(vectTrain, Y_train)\n",
        "    \n",
        "    # Append the model to the list of models\n",
        "    models.append(model)\n",
        "    vectTests.append(vectTest)\n",
        "    Y_tests.append(Y_test)"
      ],
      "metadata": {
        "id": "taHtZ9Um7u7W"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions"
      ],
      "metadata": {
        "id": "psC8428efGbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the models and their corresponding test sets\n",
        "for model, vectTest, Y_test in zip(models, vectTests, Y_tests):\n",
        "    # Make predictions on the test set\n",
        "    Y_pred = model.predict(vectTest)\n",
        "    \n",
        "    # Calculate the accuracy\n",
        "    accuracy = accuracy_score(Y_test, Y_pred)\n",
        "    \n",
        "    # Print the accuracy\n",
        "    print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jqp76NpfICn",
        "outputId": "a51c07e0-0892-45d7-edbb-8238548d9797"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9384044526901669\n",
            "Accuracy:  0.8928091397849462\n"
          ]
        }
      ]
    }
  ]
}