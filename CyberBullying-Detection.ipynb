{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtS68ZEmBWSyQq6b7Y8t+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreesh09/Detection-of-textual-CyberBullying-using-ML/blob/main/CyberBullying-Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries"
      ],
      "metadata": {
        "id": "3IABYOy3BgA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x16XC25fBjHd",
        "outputId": "10b8344c-3758-4c51-91d1-15716d277b6c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "NePLJUrjBkRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "dataset1 = pd.read_csv(\"twitter_racism_parsed_dataset.csv\")\n",
        "dataset2 = pd.read_csv(\"twitter_sexism_parsed_dataset.csv\")\n",
        "datasets = [dataset1, dataset2]"
      ],
      "metadata": {
        "id": "UqQvxE3eDRBm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove special characters, numbers, and URLs\n",
        "def remove(data):\n",
        "  data.loc[:, 'Text'] = data.loc[:, 'Text'].replace('[^a-zA-Z]', ' ', regex=True)\n",
        "  return data"
      ],
      "metadata": {
        "id": "-j5ZXscvDyN2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase the text\n",
        "def LC(data):\n",
        "  data.loc[:, 'Text'] = data.loc[:, 'Text'].apply(lambda x: x.lower())\n",
        "  return data"
      ],
      "metadata": {
        "id": "eIS0IBa7D9wA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "def removeSW(data):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  data.loc[:, 'Text'] = data.loc[:, 'Text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "  return data"
      ],
      "metadata": {
        "id": "8rtMfBaEFd3Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform stemming\n",
        "def Stem(data):\n",
        "  stemmer = SnowballStemmer('english')\n",
        "  data.loc[:, 'Text'] = data.loc[:, 'Text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\n",
        "  return data"
      ],
      "metadata": {
        "id": "3bS1aXc4FfFg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Empty(df):\n",
        "  #removing Empty values\n",
        "  df.dropna(subset=['Text'], inplace=True)\n",
        "  df.dropna(subset=['oh_label'], inplace=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "1ZnkmFgCDINH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Duplicate(df):\n",
        "  #Removing duplicates\n",
        "  df = df.drop_duplicates(subset='Text', keep='first')\n",
        "  return df"
      ],
      "metadata": {
        "id": "YdjpqqIo040i"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(dataset):\n",
        "  dataset = Empty(dataset)\n",
        "  dataset = remove(dataset)\n",
        "  dataset = LC(dataset)\n",
        "  dataset = removeSW(dataset)\n",
        "  dataset = Stem(dataset)\n",
        "  dataset = Duplicate(dataset)"
      ],
      "metadata": {
        "id": "Aa2UlWCQloJV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in datasets:\n",
        "  dataset = clean(dataset)"
      ],
      "metadata": {
        "id": "KJGZMR6DHGUO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(datasets[1].iloc[2, 2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFCa-sYlOg_K",
        "outputId": "b2770b1f-8ad5-4751-b288-6acb0f5a36a5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rt eccl everyon underestim us still well underestim judg mkr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting of Dataset"
      ],
      "metadata": {
        "id": "rZvXvib5webG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Split(data):\n",
        "  X = data.loc[:, \"Text\"].values\n",
        "  Y = data.loc[:, \"oh_label\"].values\n",
        "  X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y,test_size=0.20)\n",
        "  splits = {'X_train': X_train, 'X_test': X_test, 'Y_train': Y_train, 'Y_test': Y_test}\n",
        "  return splits\n"
      ],
      "metadata": {
        "id": "EJjDTeRi7Blc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding Categorical values"
      ],
      "metadata": {
        "id": "W8tYAneL1l12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fitVectorizer(X):\n",
        "  vectorizer = TfidfVectorizer(lowercase=False, use_idf=True)\n",
        "  vectorizer.fit(X)\n",
        "  return vectorizer\n",
        "\n",
        "def Encode(X, vectorizer):\n",
        "  vect = vectorizer.transform(X)\n",
        "  return vect"
      ],
      "metadata": {
        "id": "0H7P4T2461t3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model\n"
      ],
      "metadata": {
        "id": "BybCv6u61R96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "vectTests = []\n",
        "Y_tests = []\n",
        "vectorizers = []\n",
        "# Iterate through the datasets\n",
        "for dataset in datasets:\n",
        "    # Split the dataset into training and test sets\n",
        "    df = Split(dataset)\n",
        "    #print(df.iloc[0,0])\n",
        "    X_train = df['X_train']\n",
        "    X_test = df['X_test']\n",
        "    Y_train = df['Y_train']\n",
        "    Y_test = df['Y_test']\n",
        "\n",
        "\n",
        "    vectorizer = fitVectorizer(X_train)\n",
        "    vectTrain = Encode(X_train, vectorizer)\n",
        "    vectTest = Encode(X_test, vectorizer)\n",
        "    \n",
        "    #model = svm.SVC(kernel=\"linear\")\n",
        "    #model = LogisticRegression()\n",
        "    model = RandomForestClassifier()\n",
        "\n",
        "    #fitting training data into the algorithm \n",
        "    model.fit(vectTrain, Y_train)\n",
        "    \n",
        "    # Append the model to the list of models\n",
        "    models.append(model)\n",
        "    vectTests.append(vectTest)\n",
        "    Y_tests.append(Y_test)\n",
        "    vectorizers.append(vectorizer)"
      ],
      "metadata": {
        "id": "taHtZ9Um7u7W"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions"
      ],
      "metadata": {
        "id": "psC8428efGbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the models and their corresponding test sets\n",
        "for model, vectTest, Y_test in zip(models, vectTests, Y_tests):\n",
        "    # Make predictions on the test set\n",
        "    Y_pred = model.predict(vectTest)\n",
        "    \n",
        "    # Calculate the accuracy\n",
        "    accuracy = accuracy_score(Y_test, Y_pred)\n",
        "    \n",
        "    # Print the accuracy\n",
        "    print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jqp76NpfICn",
        "outputId": "c36422e4-ea7c-411c-845e-676cf0f5d9ec"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9257884972170687\n",
            "Accuracy:  0.8968413978494624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "AhdVwbKIHXsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while(1):\n",
        "  str = input(\"Enter a sentence: \")\n",
        "  dict = {'Text': [str], 'oh_label': [0]}\n",
        "  df = pd.DataFrame(dict)\n",
        "  df = remove(df)\n",
        "  df = LC(df)\n",
        "  df = removeSW(df)\n",
        "  dataset = Stem(df)\n",
        "  predictions = []\n",
        "\n",
        "  for model, vectorizer in zip(models, vectorizers):\n",
        "    vectDf = Encode(df['Text'], vectorizer)\n",
        "    prediction = model.predict(vectDf)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "  if(predictions[0] == 1 and predictions[1] == 1):\n",
        "    print(\"Sexist and Racist\\n\")\n",
        "  elif predictions[0] == 1:\n",
        "    print(\"Racist\\n\")\n",
        "  elif predictions[1] == 1:\n",
        "    print(\"Sexist\\n\")\n",
        "  else:\n",
        "    print(\"Not Cyber Bullying\\n\")\n",
        "  \n",
        "  c = input(\"Do you wish to continue? y/n\\n\")\n",
        "  if(c == 'n'):\n",
        "    break;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxnG23llGziX",
        "outputId": "cc68ddca-b097-4843-b1df-68af1148f08b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence: @Alfonso_AraujoG @ardiem1m @MaxBlumenthal It has nothing to do with their grandpas. It is inherited with their religion.\n",
            "Not Cyber Bullying\n",
            "\n",
            "Do you wish to continue? y/n\n",
            "y\n",
            "Enter a sentence: @Ammaawah @jm111t You are following the religion of ignorance with an illiterate prophet and you want to talk about spelling? LOL.\n",
            "Racist\n",
            "\n",
            "Do you wish to continue? y/n\n",
            "y\n",
            "Enter a sentence: @ummsuhaym @logicalmind11 Quran 8.12 would be a good example of terrorism. http://t.co/vonYOAtpfk\n",
            "Racist\n",
            "\n",
            "Do you wish to continue? y/n\n",
            "y\n",
            "Enter a sentence: Wheres the sudden death cook off? how do they die? Can we get tickets? #Mkr\n",
            "Not Cyber Bullying\n",
            "\n",
            "Do you wish to continue? y/n\n",
            "n\n"
          ]
        }
      ]
    }
  ]
}